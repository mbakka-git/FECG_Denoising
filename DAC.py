# -*- coding: utf-8 -*-
"""VAC_ECG_03.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17NLYDvF46Cb6c1x1vdKqX8WwdWpfkknn

# Neptune Tracker
"""

!pip install neptune-client

import neptune.new as neptune
run = neptune.init(
    project="mbakka-git/ECG-Kalman",
    api_token="eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiJmYWQzYTdjYy0yOTQ4LTQ3YzgtOTBhZi0yOTM5ODVjYmY3MGMifQ==",
    source_files="VAC_ECG_03.ipynb"
)

"""# VAC Classe

"""

import os
import pickle
import time

from tensorflow.keras import Model
from tensorflow.keras.layers import Input, Conv2D, Conv1D, ReLU, BatchNormalization, \
    Flatten, Dense, Reshape, Conv2DTranspose, Conv1DTranspose, Activation, Lambda
from tensorflow.keras import backend as K
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.losses import MeanSquaredError
import numpy as np
import tensorflow as tf

from tensorflow.keras.losses import BinaryCrossentropy

tf.compat.v1.disable_eager_execution()

def mse(y_target, y_predicted):
        error = y_target - y_predicted
        reconstruction_loss = K.mean(K.square(error), axis=[1,2])
        return reconstruction_loss

checkpoint_filepath = '/content/check/{epoch:02d}-{val_loss:.2f}.hdf5'
model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(
    filepath=checkpoint_filepath,
    save_weights_only=True,
    monitor='val_loss',
    mode='min',
    save_best_only=True)

class AE:
    """
    VAE represents a Deep Convolutional variational autoencoder architecture
    with mirrored encoder and decoder components.
    """

    def __init__(self,
                 input_shape,
                 conv_filters,
                 conv_kernels,
                 conv_strides,
                 latent_space_dim):
        self.input_shape = input_shape 
        self.conv_filters = conv_filters 
        self.conv_kernels = conv_kernels 
        self.conv_strides = conv_strides 
        self.latent_space_dim = latent_space_dim

        self.encoder = None
        self.decoder = None
        self.model = None

        self._encoder_output_dim = None
        self._num_conv_layers = len(conv_filters)
        self._model_input = None

        self._build()

    def summary(self):
        self.encoder.summary()
        self.decoder.summary()
        self.model.summary()

    def compile(self, learning_rate=0.0001):
        print('In compile.')
        optimizer = Adam(learning_rate=learning_rate)
        print('Optimizer set.')
        self.model.compile(optimizer=optimizer,
                           loss=self._calculate_reconstruction_loss,
                           metrics=[tf.keras.metrics.MeanSquaredError()])
        print('End of compile')

# NeptuneLogger() 
    def train(self, x_train, batch_size, num_epochs):
        return self.model.fit(x_train,
                       x_train,
                       validation_split=0.285,
                       callbacks= [model_checkpoint_callback,NeptuneLogger()],
                       batch_size = batch_size,
                       epochs=num_epochs,
                       shuffle=True)
        
    def train_with_avg(self, x_train, x_avg, batch_size, num_epochs):
        return self.model.fit(x_train,
                       x_avg,
                       validation_split=0.285,
                       callbacks= [model_checkpoint_callback,NeptuneLogger()],
                       batch_size = batch_size,
                       epochs=num_epochs,
                       shuffle=True)
        
    def save(self, save_folder="."):
        self._create_folder_if_it_doesnt_exist(save_folder)
        self._save_parameters(save_folder)
        self._save_weights(save_folder)

    def load_weights(self, weights_path):
        self.model.load_weights(weights_path)

    def reconstruct(self, images):
        latent_representations = self.encoder.predict(images)
        reconstructed_images = self.decoder.predict(latent_representations)
        return reconstructed_images, latent_representations

    @classmethod
    def load(cls, save_folder="."):
        parameters_path = os.path.join(save_folder, "parameters.pkl")
        with open(parameters_path, "rb") as f:
            parameters = pickle.load(f)
        autoencoder = AE(*parameters)
        weights_path = os.path.join(save_folder, "weights.h5")
        autoencoder.load_weights(weights_path)
        return autoencoder

    def _calculate_reconstruction_loss(self, y_target, y_predicted):
        error = y_target - y_predicted
        reconstruction_loss = K.mean(K.square(error), axis=[1, 2])
        return reconstruction_loss

    def _create_folder_if_it_doesnt_exist(self, folder):
        if not os.path.exists(folder):
            os.makedirs(folder)

    def _save_parameters(self, save_folder):
        parameters = [
            self.input_shape,
            self.conv_filters,
            self.conv_kernels,
            self.conv_strides,
            self.latent_space_dim
        ]
        save_path = os.path.join(save_folder, "parameters.pkl")
        with open(save_path, "wb") as f:
            pickle.dump(parameters, f)

    def _save_weights(self, save_folder):
        save_path = os.path.join(save_folder, "weights.h5")
        self.model.save_weights(save_path)

    def _build(self):
        self._build_encoder()
        self._build_decoder()
        self._build_autoencoder()

# Buiding AutoEncoder

    def _build_autoencoder(self):
        model_input = self._model_input
        model_output = self.decoder(self.encoder(model_input))
        self.model = Model(model_input, model_output, name="autoencoder")

# Buiding Decoder

    def _build_decoder(self):
        decoder_input = self._add_decoder_input()
        dense_layer = self._add_dense_layer(decoder_input)
        reshape_layer = self._add_reshape_layer(dense_layer)
        conv_transpose_layers = self._add_conv_transpose_layers(reshape_layer)
        decoder_output = self._add_decoder_output(conv_transpose_layers)
        self.decoder = Model(decoder_input, decoder_output, name="decoder")

    def _add_decoder_input(self):
        return Input(shape=self.latent_space_dim, name="decoder_input")

    def _add_dense_layer(self, decoder_input):
        num_neurons = np.prod(self._encoder_output_dim) # [1, 2, 4] -> 8
        dense_layer = Dense(num_neurons, name="decoder_dense")(decoder_input)
        return dense_layer

    def _add_reshape_layer(self, dense_layer):
        return Reshape(self._encoder_output_dim)(dense_layer)

    def _add_conv_transpose_layers(self, x):
        """Add conv transpose blocks."""
        # loop through all the conv layers in reverse order and stop at the
        # first layer
        for layer_index in reversed(range(1, self._num_conv_layers)):
            x = self._add_conv_transpose_layer(layer_index, x)
        return x

    def _add_conv_transpose_layer(self, layer_index, x):
        layer_num = self._num_conv_layers - layer_index
        conv_transpose_layer = Conv2DTranspose(
            filters=self.conv_filters[layer_index],
            kernel_size=self.conv_kernels[layer_index],
            strides=self.conv_strides[layer_index],
            padding="same",
            name=f"decoder_conv_transpose_layer_{layer_num}"
        )
        x = conv_transpose_layer(x)
        x = ReLU(name=f"decoder_relu_{layer_num}")(x)
        x = BatchNormalization(name=f"decoder_bn_{layer_num}")(x)
        return x

    def _add_decoder_output(self, x):
        conv_transpose_layer = Conv2DTranspose(
            filters=1,
            kernel_size=self.conv_kernels[0],
            strides=self.conv_strides[0],
            padding="same",
            name=f"decoder_conv_transpose_layer_{self._num_conv_layers}"
        )
        x = conv_transpose_layer(x)
        output_layer = Activation("sigmoid", name="sigmoid_layer")(x)
        output_layer= x
        return output_layer

# Buiding Encoder 

    def _build_encoder(self):
        encoder_input = self._add_encoder_input()
        conv_layers = self._add_conv_layers(encoder_input)
        bottleneck = self._add_bottleneck(conv_layers)
        #self._encoder_output_dim = K.int_shape(conv_layers)[1:]
        self._model_input = encoder_input
        self.encoder = Model(encoder_input, bottleneck, name="encoder")
        print('Encoder Built')

    def _add_encoder_input(self):
        return Input(shape=self.input_shape, name="encoder_input")

    def _add_conv_layers(self, encoder_input):
        """Create all convolutional blocks in encoder."""
        x = encoder_input
        for layer_index in range(self._num_conv_layers):
            x = self._add_conv_layer(layer_index, x)
        return x

    def _add_conv_layer(self, layer_index, x):
        """Add a convolutional block to a graph of layers, consisting of
        conv 2d + ReLU + batch normalization.
        """
        layer_number = layer_index + 1
        conv_layer = Conv2D(
            filters=self.conv_filters[layer_index],
            kernel_size=self.conv_kernels[layer_index],
            strides=self.conv_strides[layer_index],
            padding="same",
            name=f"encoder_conv_layer_{layer_number}"
        )
        x = conv_layer(x)
        x = ReLU(name=f"encoder_relu_{layer_number}")(x)
        x = BatchNormalization(name=f"encoder_bn_{layer_number}")(x)
        return x

    def _add_bottleneck(self, x):
        """Flatten data and add bottleneck with Guassian sampling (Dense
        layer).
        """
        self._encoder_output_dim = K.int_shape(x)[1:]
        x = Flatten()(x)
        x = Dense(self.latent_space_dim, name="latent_space")(x)
        print('Bottelneck shape=', x.shape)
        return x

"""# Import Data"""

#Training Data
import h5py

N = 20
input_length = 512
data = np.empty((6,1))
lenghts_of_files = []
lenghts_of_mod_files = []
num_peaks_ls = []
N_peaks = 1000
start_time = time.time()

file_list = ["%02d" % x for x in range(N+1)]

for i in file_list:
  with h5py.File('/content/drive/MyDrive/KalmanNetProject/meas0'+str(i)+'.data','r') as hdf:
    interm_data = np.empty((6,1))
    ls = list(hdf.keys())
    new_data = hdf.get('fetalSignal')
    new_data = np.array(new_data)
    new_data = new_data.T
    lenghts_of_files.append(new_data.shape[1])
    print("Length of file",i,"=",new_data.shape[1] )
    print(new_data.shape)
    new_peaks = hdf.get('fRpeaks')
    new_peaks = [x for x in new_peaks if str(x) != 'nan']
    new_peaks = np.array(new_peaks)
    new_peaks = new_peaks.T
    new_peaks = new_peaks.squeeze().astype(int)
    # Added this one line below
    new_peaks = new_peaks[:N_peaks]
    #num_peaks = new_peaks.shape[0] changed to 
    num_peaks = N_peaks
    if new_peaks.shape[0]< N_peaks:
      num_peaks = new_peaks.shape[0]
      for j in range(new_peaks.shape[0]):
        peak_indice = int(new_peaks[j])
        if peak_indice-235>0 and peak_indice+236<new_data.shape[1]:
          concat = new_data[:,peak_indice-235:peak_indice+236]
          concat = np.pad(concat, ((0,0),(20, 21)), 'edge')
          interm_data = np.concatenate((interm_data, concat), axis=1)
        else:
          num_peaks = num_peaks-1  
    else:
      for j in range(new_peaks.shape[0]):
        peak_indice = int(new_peaks[j])
        if peak_indice-235>0 and peak_indice+236<new_data.shape[1]:
          concat = new_data[:,peak_indice-235:peak_indice+236]
          concat = np.pad(concat, ((0,0),(20, 21)), 'edge')
          interm_data = np.concatenate((interm_data, concat), axis=1)
        else:
          num_peaks = num_peaks-1
    
    num_peaks_ls.append(num_peaks)
    print("Number of peaks in file",i,"=",num_peaks)

    interm_data = interm_data[:,1:]
    print("Data of file",i,"is of shape =",interm_data.shape)
    lenghts_of_mod_files.append(interm_data.shape[1])
    data = np.concatenate((data,interm_data), axis=1)
    print("NEXT")
    

data = data[:,1:]

print("--- Data loading time is %s seconds ---" % (time.time() - start_time))

print("Data shape=", data.shape)
print("Length of files =",lenghts_of_files)
print("Number of peaks =",num_peaks_ls)
print("Length of modified files =",lenghts_of_mod_files)

# Check if the dimensions are correct
sum = np.sum(np.array(lenghts_of_mod_files))
if (sum==data.shape[1]) : print("The sum of the final Data is coherent with the sum of the files.")
comparison =  np.array(lenghts_of_mod_files) == np.array(num_peaks_ls)*512
if comparison.all(): print("Number of peaks*512 is coherent with the modified files length.")

# Replace Nan elements in Data
data = np.nan_to_num(data)
print("Shape of data =",data.shape)

print('The number of nan elements after replacement = ',np.count_nonzero(np.isnan(data)))

# Create a moving average of the ecg signals

from scipy import signal

N_avg = 30
def average_ecg(data):

  x = data.shape[0]
  y = data.shape[1]

  augmented_data = np.concatenate((data,data[:,-512*N_avg:-512]), axis=1)
  one_positions = np.arange(0,512*N_avg,512)
  window = np.zeros(512*N_avg)
  window[one_positions] = 1

  data_avg = np.zeros((x,y))
  for i in range(6):
    intermed = signal.convolve(data[i,:],window)/N_avg
    data_avg[i] = intermed[512*(N_avg-1):512*(N_avg-1)+y]

  return data_avg

avg_ecg = average_ecg(data)
print(avg_ecg.shape)

# Visualise averaged ecg
import matplotlib.pyplot as plt
from matplotlib.pyplot import figure

figure(figsize=(15, 8), dpi=80)
plt.plot(data[0,:512*5])
plt.title('Original ECG')
plt.show()

figure(figsize=(15, 8), dpi=80)
plt.plot(avg_ecg[0,:512*5])
plt.title('Averaged ECG')
plt.show()

import matplotlib.pyplot as plt
plt.plot(data[0,512*100:512*101])
plt.title('Original signal')
plt.show()
plt.plot(avg_ecg[0,512*100:512*101])
plt.title('Averaged signal')

# Reshape data + split it into train and test sets.
from sklearn.model_selection import train_test_split

number_inputs = int(data.shape[1]/input_length)

def modify_shape(array, input_length):
  '''
    Modify array shape from (6, n of samples) -----> (n of heart beats, 6, 512, 1)
    n of heart beats = n of samples/512.
    512 = number of samples per hear beat.
  '''
  if array.ndim !=2:
    raise Exception("Sorry, the number of dimension of the given array must be 2.")
  if array.shape[0] != 6:
    raise Exception("Sorry, the array must contain 6 channels.")
  
  dim = array.shape
  print("Modify Shape func : the given array is of dim =",dim)
  n_hbeats = int(dim[1]/512)
  array = np.stack(np.split(array, np.arange(input_length,dim[1],input_length), axis=1))
  array = array.reshape(n_hbeats,6*input_length,1)
  array = array.reshape(array.shape[0],6,512,1)
  return array 

ecg = data[:,0:input_length*number_inputs]

#ecg= ecg.reshape(6*number_inputs,input_length,1) Modified to 
'''
ecg = np.stack(np.split(ecg, np.arange(input_length,data.shape[1],input_length), axis=1))
ecg = ecg.reshape(np.sum(np.array(num_peaks_ls)),6*input_length,1)
'''
ecg = modify_shape(ecg,input_length)
ecg_train, ecg_test = train_test_split(ecg, test_size=0.3, random_state=42)

#avg_ecg_train = average_ecg(ecg_train)
#avg_ecg_test = average_ecg(ecg_test)

print("ECG total shape:", ecg.shape)
print("ECG training shape:", ecg_train.shape)
print("ECG test shape:", ecg_test.shape)

# Create average for training set
def to_original_dim(ecg_test):
  ecg_test = ecg_test.squeeze()
  dim = ecg_test.shape
  ecg_test_list = np.split(ecg_test, np.arange(1,dim[0]), axis=0)
  ecg_test_list = [item.squeeze() for item in ecg_test_list]
  
  return np.hstack(ecg_test_list)

ecg_train_avg = average_ecg(to_original_dim(ecg_train))
print("ECG train average shape =",ecg_train_avg.shape)

ecg_train_avg = modify_shape(ecg_train_avg, input_length)
print("ECG train average shape =",ecg_train_avg.shape)

# Create average for testing set

ecg_test_avg = average_ecg(to_original_dim(ecg_test))
print("ECG test average shape =",ecg_test_avg.shape)

ecg_test_avg = modify_shape(ecg_test_avg, input_length)
print("ECG test average shape =",ecg_test_avg.shape)

# Replace nan in the data sets.
ecg_train = np.nan_to_num(ecg_train)
print("Shape of ecg_train =",ecg_train.shape)
ecg_test = np.nan_to_num(ecg_test)
print("Shape of ecg_test =",ecg_test.shape)

print('The number of nan elements after replacement = ',np.count_nonzero(np.isnan(ecg_train)))

"""# Main

## Train
"""

from tensorflow import keras
class NeptuneLogger(keras.callbacks.Callback):
  def on_epoch_end(self,epoch,logs={}):
    for log_name, log_value in logs.items():
      run['epoch/{}'.format(log_name)].log(log_value)

LEARNING_RATE = 0.0001
BATCH_SIZE = 32
EPOCHS = 100
LATENT_SPACE = 25

def train(x_train, learning_rate, batch_size, epochs):
    autoencoder = AE(
        input_shape=(6, 512, 1),
        conv_filters=(40, 20, 20, 20, 20, 40),
        conv_kernels=([6,40], [6,40], [6,40], [6,40], [6,40], [6,40]),
        conv_strides=([1,2], [1,2], [1,2], [1,2], [1,2], [1,2]),
        latent_space_dim = LATENT_SPACE)
    autoencoder.summary()
    print('Before Compile')
    autoencoder.compile(learning_rate)
    print('Compiled')
    history = autoencoder.train(x_train, batch_size, epochs)
    return autoencoder, history
  
def train_with_avg(x_train, x_train_avg, learning_rate, batch_size, epochs):
    autoencoder = AE(
        input_shape=(6, 512, 1),
        conv_filters=(40, 20, 20, 20, 20, 40),
        conv_kernels=([6,40], [6,40], [6,40], [6,40], [6,40], [6,40]),
        conv_strides=([1,2], [1,2], [1,2], [1,2], [1,2], [1,2]),
        latent_space_dim = LATENT_SPACE)
    autoencoder.summary()
    print('Before Compile')
    autoencoder.compile(learning_rate)
    print('Compiled')
    history = autoencoder.train_with_avg(x_train, x_train_avg, batch_size, epochs)
    return autoencoder, history
  
start_time = time.time()
autoencoder, history = train_with_avg(ecg_train, ecg_train_avg, LEARNING_RATE, BATCH_SIZE, EPOCHS)
print("--- Training time is %s sec ---" % (time.time() - start_time))
autoencoder.save("model")

autoencoder.model.save('netron_model.h5')

# RUN IT IF YOU WANT TO SAVE THE MODEL PARAMETERS 
#autoencoder.save("model")

print(history.history.keys())

# LOSS PLOT
import matplotlib.pyplot as plt
from matplotlib.pyplot import figure
figure(figsize=(8, 6), dpi=80)
log_loss = 10*np.log10(np.array(history.history['loss'])/(512*6))
plt.plot(log_loss)
#plt.plot(history.history['val_mse'])
plt.title('model loss')
plt.ylabel('10*log(loss)')
plt.xlabel('epoch')
plt.legend(['Training loss'], loc='upper left')
plt.savefig('Training_loss.png', bbox_inches='tight')
plt.show()

figure(figsize=(8, 6), dpi=80)
#plt.plot(history.history['loss'])
log_loss = 10*np.log10(np.array(history.history['val_mean_squared_error'])/(512*6))
plt.plot(log_loss, color='red')
plt.title('model loss')
plt.ylabel('10*log(loss)')
plt.xlabel('epoch')
plt.legend(['Validation loss'], loc='upper left')
plt.savefig('Validation_loss.png', bbox_inches='tight')
plt.show()

"""## Load Model (Run only if the model was not trained)"""

#autoencoder = AE.load("model")

#print(history.history.keys())

"""# Ploting Results"""

signals = []
reconstructed_signals = []
for i in range(10):
  reconstructed = autoencoder.reconstruct(ecg_test[i:i+1,:])
  reconstructed = np.asarray(reconstructed[0])
  reconstructed = reconstructed.reshape(512*6,1)
  signals.append(ecg_test_avg[i:i+1,:].reshape(512*6,1))
  reconstructed_signals.append(reconstructed)

for i in range(10):
  """
  fig, axis = plt.subplots(1,1,figsize=(15,3))
  axis[0].plot(signals[i])
  axis[1].plot(reconstructed_signals[i])
  axis[0].set_title('Signal')
  axis[1].set_title('Reconstructed Signal')
  """
  figure(figsize=(8, 6), dpi=80)
  plt.plot(signals[i][:512],'b')
  plt.plot(reconstructed_signals[i][:512],'g')
  plt.title('Averaged and reconstructed signal after VAC')
  plt.xlabel('Samples')
  plt.legend(['Averaged','Reconstructed'], loc='upper left')
  plt.savefig('test_10_'+str(i)+'.png', bbox_inches='tight')
  plt.show()

"""# Compute performance

"""

# Compare performances

# test_reconstructions, ecg_test ----> ecg_test_avg

def performance(signal, estimation):
  if signal.shape != estimation.shape:
    raise Exception("The given array dimensions must be the same.")

  signal = signal.squeeze()
  estimation = estimation.squeeze()

  diff = signal-estimation
  epsilon = np.sum(np.linalg.norm(diff, axis=2)**2, axis=0)/ np.sum(np.linalg.norm(signal, axis=2)**2, axis=0)
  epsilon = np.squeeze(epsilon)
  epsilon = np.sum(epsilon)/signal.shape[1]

  return epsilon

test_reconstructions = autoencoder.reconstruct(ecg_test)
test_reconstructions = np.asarray(test_reconstructions[0])

error_init = performance(ecg_test_avg,ecg_test)
error_model = performance(ecg_test_avg,test_reconstructions)

print('test_reconstructions shape=', test_reconstructions.shape)
print('avg_ecg shape=', test_reconstructions.shape)
print('ecg_test shape=', test_reconstructions.shape)

print('error_init =',error_init)
print('error_model =',error_model)

"""# Save Files

"""

!zip -r /content/model_.zip netron_model.h5 model check \
/content/test_10_0.png /content/test_10_1.png /content/test_10_2.png \
/content/test_10_3.png /content/test_10_4.png /content/test_10_5.png \
/content/test_10_6.png /content/test_10_7.png /content/test_10_8.png \
/content/test_10_9.png /content/Validation_loss.png /content/Training_loss.png

from google.colab import files
files.download("/content/model_.zip")